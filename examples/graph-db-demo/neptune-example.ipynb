{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune as Graph Memory\n",
    "\n",
    "In this notebook, we will be connecting using a AWS Neptune Analytics instance as our memory graph storage for Mem0.\n",
    "\n",
    "The Graph Memory storage persists memories in a graph or relationship form when performing `m.add` memory operations. It then uses vector distance algorithms to find related memories during a `m.search` operation. Relationships are returned in the result, and add context to the memories.\n",
    "\n",
    "References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### 1. Install Mem0 with Graph Memory support \n",
    "\n",
    "To use Mem0 with Graph Memory support, install it using pip:\n",
    "\n",
    "```bash\n",
    "pip install \"mem0ai[graph]\"\n",
    "```\n",
    "\n",
    "This command installs Mem0 along with the necessary dependencies for graph functionality.\n",
    "\n",
    "### 2. Connect to Neptune\n",
    "\n",
    "To connect to AWS Neptune Analytics, you need to configure Neptune with your AWS profile credentials. The best way to do this is to declare environment variables with IAM permission to your Neptune Analytics instance. The `graph-identifier` for the instance to persist memories needs to be defined in the Mem0 configuration under `\"graph_store\"`, with the `\"neptune\"` provider.  Note that the Neptune Analytics instance needs to have `vector-search-configuration` defined to meet the needs of the llm model's vector dimensions.\n",
    "\n",
    "```python\n",
    "embedding_dimensions = 1536\n",
    "graph_identifier = \"<MY-GRAPH>\" # graph with 1536 dimensions for vector search\n",
    "config = {\n",
    "    \"embedder\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"config\": {\n",
    "            \"model\": \"text-embedding-3-large\",\n",
    "            \"embedding_dims\": embedding_dimensions\n",
    "        },\n",
    "    },\n",
    "    \"graph_store\": {\n",
    "        \"provider\": \"neptune\",\n",
    "        \"config\": {\n",
    "            \"graph_identifier\": graph_identifier,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. Configure OpenSearch\n",
    "\n",
    "We're going to use OpenSearch as our vector store.  You can run [OpenSearch from docker image](https://docs.opensearch.org/docs/latest/install-and-configure/install-opensearch/docker/):\n",
    "\n",
    "```bash\n",
    "docker pull opensearchproject/opensearch:3\n",
    "```\n",
    "\n",
    "And verify that it's running with a `<custom-admin-password>`:\n",
    "\n",
    "```bash\n",
    " docker run -d -p 9200:9200 -p 9600:9600 -e \"discovery.type=single-node\" -e \"OPENSEARCH_INITIAL_ADMIN_PASSWORD=<custom-admin-password>\" opensearchproject/opensearch:latest\n",
    "\n",
    " curl https://localhost:9200 -ku admin:<custom-admin-password>\n",
    "```\n",
    "\n",
    "We're going to connect [OpenSearch using the python client](https://github.com/opensearch-project/opensearch-py):\n",
    "\n",
    "```bash\n",
    "pip install \"opensearch-py\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Do all the imports and configure OpenAI (enter your OpenAI API key):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from mem0 import Memory\n",
    "from opensearchpy import OpenSearch\n",
    "from langchain_aws import NeptuneAnalyticsGraph\n",
    "import os, logging, sys\n",
    "\n",
    "logging.getLogger('mem0.memory.neptune_memory').setLevel(logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    stream=sys.stdout  # Explicitly set output to stdout\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Setup the Mem0 configuration using:\n",
    "- openai as the embedder\n",
    "- AWS Neptune Analytics instance as a graph store\n",
    "- OpenSearch as the vector store"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "graph_identifier = os.environ.get(\"GRAPH_ID\")\n",
    "opensearch_username = os.environ.get(\"OS_USERNAME\")\n",
    "opensearch_password = os.environ.get(\"OS_PASSWORD\")\n",
    "config = {\n",
    "    \"embedder\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"config\": {\n",
    "            \"model\": \"text-embedding-3-large\",\n",
    "            \"embedding_dims\": 1536\n",
    "        },\n",
    "    },\n",
    "    \"graph_store\": {\n",
    "        \"provider\": \"neptune\",\n",
    "        \"config\": {\n",
    "            \"graph_identifier\": graph_identifier,\n",
    "        },\n",
    "    },\n",
    "    \"vector_store\": {\n",
    "        \"provider\": \"opensearch\",\n",
    "        \"config\": {\n",
    "            \"collection_name\": \"vector_store\",\n",
    "            \"host\": \"localhost\",\n",
    "            \"port\": 9200,\n",
    "            \"user\": opensearch_username,\n",
    "            \"password\": opensearch_password,\n",
    "            \"use_ssl\": False,\n",
    "            \"verify_certs\": False,\n",
    "        },\n",
    "    },\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Graph Memory initializiation\n",
    "\n",
    "Initialize Memgraph as a Graph Memory store:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "m = Memory.from_config(config_dict=config)\n",
    "\n",
    "app_id = \"movies\"\n",
    "user_id = \"alice\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Store memories\n",
    "\n",
    "Create memories and store:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"How about a thriller movies? They can be quite engaging.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Store inferred memories (default behavior)\n",
    "result = m.add(messages, user_id=user_id, metadata={\"category\": \"movie_recommendations\"})\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search memories"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "search_results = m.search(\"what does alice love?\", user_id=user_id)\n",
    "for result in search_results[\"results\"]:\n",
    "    print(f\"\\\"{result[\"memory\"]}\\\" [score: {result[\"score\"]}]\")\n",
    "for relation in search_results[\"relations\"]:\n",
    "    print(f\"{relation}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Neptune Analytics Graph"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_results = m.get_all(user_id=user_id)\n",
    "for n in all_results['results']:\n",
    "    print(f\"node \\\"{n[\"memory\"]}\\\": [hash: {n[\"hash\"]}]\")\n",
    "\n",
    "for e in all_results['relations']:\n",
    "    print(f\"edge \\\"{e[\"source\"]}\\\" --{e[\"relationship\"]}--> \\\"{e[\"target\"]}\\\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "neptune_graph = NeptuneAnalyticsGraph(graph_identifier)\n",
    "\n",
    "query = \"\"\"\n",
    "        MATCH (n {user_id: $user_id})-[r]->(m {user_id: $user_id})\n",
    "        RETURN n.name AS source, type(r) AS relationship, m.name AS target\n",
    "        LIMIT $limit\n",
    "        \"\"\"\n",
    "edge_results = neptune_graph.query(\n",
    "    query, params={\"user_id\": user_id, \"limit\": 100}\n",
    ")\n",
    "\n",
    "print(\"----RELATIONSHIPS----\")\n",
    "for e in edge_results:\n",
    "    print(f\"edge \\\"{e[\"source\"]}\\\" --{e[\"relationship\"]}--> \\\"{e[\"target\"]}\\\"\")\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (n {user_id: $user_id})\n",
    "CALL neptune.algo.vectors.get(n)\n",
    "YIELD embedding\n",
    "RETURN n AS node, embedding\n",
    "LIMIT $limit\n",
    "\"\"\"\n",
    "\n",
    "node_results = neptune_graph.query(\n",
    "    query, params={\"user_id\": user_id, \"limit\": 100}\n",
    ")\n",
    "# print(f\"node_results={node_results}\")\n",
    "\n",
    "print(\"----NODES----\")\n",
    "for n in node_results:\n",
    "    has_embedding = n.get(\"embedding\", None) is not None\n",
    "    print(f\"node name:\\\"{n[\"node\"][\"~properties\"][\"name\"]}\\\" embedding: {has_embedding}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "opensearch_index = \"vector_store\"\n",
    "vector_store_config = config[opensearch_index][\"config\"]\n",
    "\n",
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': vector_store_config[\"host\"], 'port': vector_store_config[\"port\"]}],\n",
    "    http_compress = True, # enables gzip compression for request bodies\n",
    "    http_auth = (vector_store_config[\"user\"], vector_store_config[\"password\"]),\n",
    "    use_ssl = vector_store_config[\"use_ssl\"],\n",
    "    verify_certs = vector_store_config[\"verify_certs\"],\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False,\n",
    ")\n",
    "\n",
    "query = {\"query\":{\"match_all\": {}}}\n",
    "\n",
    "response = client.search(\n",
    "    body = query,\n",
    "    index = opensearch_index,\n",
    ")\n",
    "\n",
    "print(\"----VECTORS----\")\n",
    "for v in response[\"hits\"][\"hits\"]:\n",
    "    print(f\"vector id=\\\"{v[\"_source\"][\"id\"]}\\\" data={v[\"_source\"][\"payload\"][\"data\"]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "m.reset()\n",
    "# only works for neptune_memory\n",
    "memory_graph = m.graph.reset()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
